{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MIGHT data - real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (352, 2524)\n",
      "Number of samples (rows): 352\n",
      "Number of columns: 2524\n",
      "Label 'Cancer Status' distribution:\n",
      "Cancer Status\n",
      "0    250\n",
      "1    102\n",
      "Name: count, dtype: int64\n",
      "Feature columns (excluding label):\n",
      "['1:1000001-2000000', '1:3000001-4000000', '1:4000001-5000000', '1:5000001-6000000', '1:6000001-7000000', '1:7000001-8000000', '1:8000001-9000000', '1:9000001-10000000', '1:10000001-11000000', '1:11000001-12000000'] ...\n",
      "Train model on 352 examples\n",
      "Model trained in 0:00:00.135285\n",
      "\n",
      "[INFO] Yggdrasil RF trained in 0.55 seconds.\n",
      "\n",
      "Evaluation metrics on the training set:\n",
      "accuracy: 1\n",
      "confusion matrix:\n",
      "    label (row) \\ prediction (col)\n",
      "    +-----+-----+-----+\n",
      "    |     |   0 |   1 |\n",
      "    +-----+-----+-----+\n",
      "    |   0 | 250 |   0 |\n",
      "    +-----+-----+-----+\n",
      "    |   1 |   0 | 102 |\n",
      "    +-----+-----+-----+\n",
      "characteristics:\n",
      "    name: '1' vs others\n",
      "    ROC AUC: 1\n",
      "    PR AUC: 1\n",
      "    Num thresholds: 58\n",
      "loss: 0.164877\n",
      "num examples: 352\n",
      "num examples (weighted): 352\n",
      "\n",
      "\n",
      "Sample predictions (first 5 rows):\n",
      "[0.7099996  0.9399994  0.78999954 0.87999946 0.78999954 0.68999964\n",
      " 0.78999954 0.88999945 0.7399996  0.77999955 0.87999946 0.65999967\n",
      " 0.65999967 0.78999954 0.79999954 0.69999963 0.7499996  0.8099995\n",
      " 0.8399995  0.7499996  0.8299995  0.68999964 0.16       0.14999999\n",
      " 0.12999998 0.6099997  0.77999955 0.69999963 0.66999966 0.06999999\n",
      " 0.08999999 0.7099996  0.68999964 0.6099997  0.7299996  0.7499996\n",
      " 0.7499996  0.06999999 0.04       0.06999999 0.06999999 0.04\n",
      " 0.05999999 0.09999999 0.08999999 0.07999999 0.10999998 0.05999999\n",
      " 0.08999999 0.09999999 0.11999998 0.05       0.08999999 0.07999999\n",
      " 0.65999967 0.8399995  0.16       0.10999998 0.04       0.22000003\n",
      " 0.04       0.07999999 0.06999999 0.02       0.05999999 0.08999999\n",
      " 0.14999999 0.08999999 0.10999998 0.07999999 0.09999999 0.06999999\n",
      " 0.10999998 0.12999998 0.08999999 0.11999998 0.12999998 0.06999999\n",
      " 0.09999999 0.11999998 0.08999999 0.7399996  0.13999999 0.05999999\n",
      " 0.05       0.09999999 0.12999998 0.20000002 0.08999999 0.07999999\n",
      " 0.10999998 0.65999967 0.17       0.86999947 0.8099995  0.7399996\n",
      " 0.7299996  0.7599996  0.09999999 0.05999999 0.08999999 0.05\n",
      " 0.09999999 0.10999998 0.17       0.06999999 0.7199996  0.09999999\n",
      " 0.09999999 0.09999999 0.10999998 0.05       0.08999999 0.09999999\n",
      " 0.08999999 0.08999999 0.07999999 0.08999999 0.07999999 0.11999998\n",
      " 0.06999999 0.12999998 0.06999999 0.09999999 0.12999998 0.07999999\n",
      " 0.13999999 0.07999999 0.05       0.06999999 0.14999999 0.12999998\n",
      " 0.04       0.12999998 0.11999998 0.05999999 0.05999999 0.06999999\n",
      " 0.07999999 0.08999999 0.08999999 0.08999999 0.10999998 0.05\n",
      " 0.05999999 0.07999999 0.08999999 0.09999999 0.22000003 0.06999999\n",
      " 0.6499997  0.14999999 0.06999999 0.05999999 0.07999999 0.05\n",
      " 0.08999999 0.06999999 0.02       0.17       0.08999999 0.69999963\n",
      " 0.7599996  0.13999999 0.11999998 0.8499995  0.07999999 0.14999999\n",
      " 0.07999999 0.7099996  0.79999954 0.07999999 0.13999999 0.11999998\n",
      " 0.05999999 0.17       0.13999999 0.8399995  0.08999999 0.66999966\n",
      " 0.5299998  0.57999974 0.5999997  0.12999998 0.05       0.16\n",
      " 0.6399997  0.05999999 0.06999999 0.10999998 0.79999954 0.05\n",
      " 0.10999998 0.14999999 0.11999998 0.09999999 0.07999999 0.05999999\n",
      " 0.10999998 0.06999999 0.05       0.13999999 0.12999998 0.11999998\n",
      " 0.7599996  0.09999999 0.69999963 0.5999997  0.77999955 0.05\n",
      " 0.06999999 0.05999999 0.08999999 0.09999999 0.05       0.05999999\n",
      " 0.77999955 0.07999999 0.11999998 0.05999999 0.8199995  0.08999999\n",
      " 0.05       0.07999999 0.08999999 0.14999999 0.02       0.12999998\n",
      " 0.14999999 0.09999999 0.05999999 0.03       0.24000004 0.10999998\n",
      " 0.11999998 0.17       0.06999999 0.12999998 0.21000002 0.10999998\n",
      " 0.04       0.08999999 0.05       0.16       0.08999999 0.07999999\n",
      " 0.05       0.04       0.10999998 0.06999999 0.09999999 0.08999999\n",
      " 0.07999999 0.05       0.14999999 0.11999998 0.6499997  0.77999955\n",
      " 0.6299997  0.66999966 0.58999974 0.55999976 0.67999965 0.57999974\n",
      " 0.06999999 0.07999999 0.17       0.06999999 0.13999999 0.22000003\n",
      " 0.09999999 0.09999999 0.10999998 0.11999998 0.08999999 0.07999999\n",
      " 0.06999999 0.11999998 0.05999999 0.16       0.08999999 0.13999999\n",
      " 0.07999999 0.12999998 0.07999999 0.18       0.10999998 0.06999999\n",
      " 0.12999998 0.13999999 0.09999999 0.04       0.12999998 0.09999999\n",
      " 0.11999998 0.05999999 0.08999999 0.06999999 0.12999998 0.11999998\n",
      " 0.05999999 0.8399995  0.7599996  0.6499997  0.11999998 0.05\n",
      " 0.12999998 0.77999955 0.05       0.88999945 0.88999945 0.7599996\n",
      " 0.8099995  0.79999954 0.9199994  0.7399996  0.6199997  0.7299996\n",
      " 0.8099995  0.7199996  0.79999954 0.8299995  0.69999963 0.65999967\n",
      " 0.8499995  0.7499996  0.8499995  0.6299997  0.56999975 0.58999974\n",
      " 0.7399996  0.8499995  0.66999966 0.77999955 0.7199996  0.6299997\n",
      " 0.8299995  0.7299996  0.14999999 0.07999999 0.09999999 0.10999998\n",
      " 0.11999998 0.13999999 0.13999999 0.13999999 0.14999999 0.18\n",
      " 0.08999999 0.12999998 0.17       0.10999998]\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# 1) Imports and Basic Setup\n",
    "###############################################\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This is the current Python package for Yggdrasil Decision Forests.\n",
    "import ydf\n",
    "\n",
    "###############################################\n",
    "# 2) Load the Exact Same Processed Data\n",
    "###############################################\n",
    "# In your MIGHT notebook, you saved something like \"processed_wise1_data.csv\".\n",
    "# Let's read that in so YDF sees the identical data.\n",
    "\n",
    "PROCESSED_DATA = \"processed_wise1_data.csv\"\n",
    "df = pd.read_csv(PROCESSED_DATA)\n",
    "\n",
    "# Suppose the label column is named \"Cancer Status\".\n",
    "LABEL_COL = \"Cancer Status\"\n",
    "\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "# print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "###############################################\n",
    "# 3) Verify We Have the Same Classification Task\n",
    "###############################################\n",
    "# Ensure that the label distribution matches what you see in MIGHT (e.g., y.value_counts()).\n",
    "\n",
    "num_rows, num_cols = df.shape\n",
    "if LABEL_COL not in df.columns:\n",
    "    raise ValueError(f\"Label column {LABEL_COL!r} not found in CSV columns!\")\n",
    "\n",
    "print(\"Number of samples (rows):\", num_rows)\n",
    "print(\"Number of columns:\", num_cols)\n",
    "print(f\"Label '{LABEL_COL}' distribution:\\n{df[LABEL_COL].value_counts()}\")\n",
    "\n",
    "# Optional: Print the first few columns to confirm\n",
    "print(\"Feature columns (excluding label):\")\n",
    "feature_cols = [c for c in df.columns if c != LABEL_COL]\n",
    "print(feature_cols[:10], \"...\" if len(feature_cols) > 10 else \"\")\n",
    "\n",
    "###############################################\n",
    "# 4) Train a YDF Random Forest\n",
    "###############################################\n",
    "# Provide the label name, and optionally set random_seed for reproducibility.\n",
    "\n",
    "rf_learner = ydf.RandomForestLearner(\n",
    "    label=LABEL_COL,\n",
    "    random_seed=42,  # ensures reproducible random sampling\n",
    "    num_trees=100,   # match MIGHT for fair comparison\n",
    "    max_depth=10     # or whatever matches your MIGHT hyperparams\n",
    ")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "rf_model = rf_learner.train(df)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "train_time = end_time - start_time\n",
    "print(f\"\\n[INFO] Yggdrasil RF trained in {train_time:.2f} seconds.\")\n",
    "\n",
    "###############################################\n",
    "# 5) Evaluate / Inspect the Model\n",
    "###############################################\n",
    "# Evaluate on the same data (for quick demonstration).\n",
    "evaluation = rf_model.evaluate(df)\n",
    "print(\"\\nEvaluation metrics on the training set:\")\n",
    "print(evaluation)\n",
    "\n",
    "# If you want, you can also get predictions or partial-dependence analysis:\n",
    "predictions = rf_model.predict(df)\n",
    "print(\"\\nSample predictions (first 5 rows):\")\n",
    "print(predictions)\n",
    "\n",
    "###############################################\n",
    "# 6) Confirm Similarity with MIGHT\n",
    "###############################################\n",
    "# For a fair side-by-side timing:\n",
    "# - MIGHT uses the same \"processed_wise1_data.csv\".\n",
    "# - Both have 100 trees, same random seed if possible.\n",
    "# - Start/stop the timer at the exact training call.\n",
    "#\n",
    "# Now the only difference should be the algorithms/impl details themselves.\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (246, 2524)\n",
      "Test set shape: (106, 2524)\n",
      "Train model on 246 examples\n",
      "Model trained in 0:00:00.107743\n",
      "\n",
      "YDF model trained in 0.17 seconds on 246 examples.\n",
      "\n",
      "=== YDF Evaluate() on Test Set ===\n",
      "accuracy: 0.811321\n",
      "confusion matrix:\n",
      "    label (row) \\ prediction (col)\n",
      "    +----+----+----+\n",
      "    |    |  0 |  1 |\n",
      "    +----+----+----+\n",
      "    |  0 | 75 |  0 |\n",
      "    +----+----+----+\n",
      "    |  1 | 20 | 11 |\n",
      "    +----+----+----+\n",
      "characteristics:\n",
      "    name: '1' vs others\n",
      "    ROC AUC: 0.943871\n",
      "    PR AUC: 0.891002\n",
      "    Num thresholds: 46\n",
      "loss: 0.426873\n",
      "num examples: 106\n",
      "num examples (weighted): 106\n",
      "\n",
      "\n",
      "=== Scikit-learn metrics on Test Set ===\n",
      "Accuracy: 0.811\n",
      "Confusion matrix:\n",
      " [[75  0]\n",
      " [20 11]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88        75\n",
      "           1       1.00      0.35      0.52        31\n",
      "\n",
      "    accuracy                           0.81       106\n",
      "   macro avg       0.89      0.68      0.70       106\n",
      "weighted avg       0.85      0.81      0.78       106\n",
      "\n",
      "\n",
      "Sample predicted probabilities (first 10):\n",
      "[0.12999998 0.32999995 0.66999966 0.06999999 0.19000001 0.16\n",
      " 0.46999982 0.43999985 0.29999998 0.28      ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import ydf  # Yggdrasil Decision Forests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 1) Read the same processed CSV as MIGHT\n",
    "CSV_FILE = \"processed_wise1_data.csv\"\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "LABEL_COL = \"Cancer Status\"\n",
    "if LABEL_COL not in df.columns:\n",
    "    raise ValueError(f\"Missing label column {LABEL_COL!r} in CSV.\")\n",
    "\n",
    "# 2) Train/test split (hold-out)\n",
    "#    We'll separate 30% of the data for testing. Use the same random_state so you can replicate in MIGHT.\n",
    "X = df.drop(columns=[LABEL_COL])\n",
    "y = df[LABEL_COL]\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "\n",
    "# 3) Build a YDF RandomForest with ~the same hyperparams as MIGHT\n",
    "#    e.g. 100 trees, random_seed=23 or 42, etc.\n",
    "rf_learner = ydf.RandomForestLearner(\n",
    "    label=LABEL_COL,\n",
    "    random_seed=42,\n",
    "    num_trees=100 #100000\n",
    ")\n",
    "\n",
    "# 4) Train on the HOLD-OUT train set\n",
    "start_time = time.perf_counter()\n",
    "rf_model = rf_learner.train(train_df)  # Only pass the train portion\n",
    "end_time = time.perf_counter()\n",
    "train_time = end_time - start_time\n",
    "print(f\"\\nYDF model trained in {train_time:.2f} seconds on {len(train_df)} examples.\")\n",
    "\n",
    "# 5) Evaluate on the test set using YDF's built-in evaluate()\n",
    "evaluation = rf_model.evaluate(test_df)\n",
    "print(\"\\n=== YDF Evaluate() on Test Set ===\")\n",
    "print(evaluation)\n",
    "\n",
    "#   By default, it prints metrics like \"accuracy\", \"confusion matrix\", \"ROC AUC\",\n",
    "#   etc. in a structured text output.\n",
    "\n",
    "# 6) (Optional) Compute scikit-learn metrics on the test set\n",
    "#    The YDF model's `.predict()` returns a DataFrame with probabilities for label=1 by default.\n",
    "preds = rf_model.predict(test_df)  # shape: (num_test_examples,) containing probabilities\n",
    "preds_np = preds#.to_numpy().ravel()\n",
    "\n",
    "# Convert probabilities -> predicted class using 0.5 threshold\n",
    "pred_labels = (preds_np >= 0.5).astype(int)\n",
    "\n",
    "test_y = test_df[LABEL_COL].values\n",
    "\n",
    "acc = accuracy_score(test_y, pred_labels)\n",
    "cm = confusion_matrix(test_y, pred_labels)\n",
    "cls_rpt = classification_report(test_y, pred_labels)\n",
    "\n",
    "print(\"\\n=== Scikit-learn metrics on Test Set ===\")\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", cls_rpt)\n",
    "\n",
    "# 7) Display sample predicted probabilities (first 10), to mimic the style you saw\n",
    "print(\"\\nSample predicted probabilities (first 10):\")\n",
    "print(preds_np[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Random Data\n",
    "\n",
    "## Oblique Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set num_attributes to 128\n",
    "\n",
    "Switched to this from 160 bcs. I switched to base-2\n",
    "\n",
    "ðŸ”¬ <font color=\"purple\">This will test Cache performance - at first 160/160 features - dense access. Then 160/320 features - 50% sparse access ... </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import ydf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "# 4) YDF RandomForest\n",
    "est = ydf.RandomForestLearner(\n",
    "    label=\"label\",\n",
    "    random_seed=random_seed,\n",
    "    split_axis=\"SPARSE_OBLIQUE\",\n",
    "    num_trees=1000,\n",
    "    bootstrap_training_dataset=True,\n",
    "    bootstrap_size_ratio=1.0,\n",
    "    num_threads=96,\n",
    "    max_depth=-1,\n",
    "\n",
    "    sparse_oblique_max_num_projections = 128,\n",
    "    # sparse_oblique_num_projections_exponent=0.0, # \"\"\" This should make max_num_projections always WIN \"\"\"\n",
    "\n",
    "    # ''' Should skip computing OOB after training'''\n",
    "    honest=False,\n",
    "    compute_oob_performances=False, # Should skip computing OOB after training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 89 examples\n",
      "Model trained in 0:00:00.051210\n",
      "\n",
      "(n=128, p=128):\n",
      "  train_time mean=0.0584s, std=0.0000\n",
      "  inference_time mean=0.0167s, std=0.0000\n",
      "  accuracy mean=0.513, std=0.000\n",
      "Train model on 89 examples\n",
      "Model trained in 0:00:00.075808\n",
      "\n",
      "(n=128, p=256):\n",
      "  train_time mean=0.0950s, std=0.0000\n",
      "  inference_time mean=0.0204s, std=0.0000\n",
      "  accuracy mean=0.564, std=0.000\n",
      "Train model on 89 examples\n",
      "Model trained in 0:00:00.134765\n",
      "\n",
      "(n=128, p=512):\n",
      "  train_time mean=0.3912s, std=0.0000\n",
      "  inference_time mean=0.0310s, std=0.0000\n",
      "  accuracy mean=0.410, std=0.000\n",
      "Train model on 89 examples\n",
      "Model trained in 0:00:00.242387\n",
      "\n",
      "(n=128, p=1024):\n",
      "  train_time mean=0.2932s, std=0.0000\n",
      "  inference_time mean=0.0515s, std=0.0000\n",
      "  accuracy mean=0.538, std=0.000\n",
      "Train model on 89 examples\n",
      "Model trained in 0:00:00.472674\n",
      "\n",
      "(n=128, p=2048):\n",
      "  train_time mean=0.5662s, std=0.0000\n",
      "  inference_time mean=0.0949s, std=0.0000\n",
      "  accuracy mean=0.487, std=0.000\n",
      "Train model on 89 examples\n",
      "Model trained in 0:00:00.885311\n",
      "\n",
      "(n=128, p=4096):\n",
      "  train_time mean=1.0953s, std=0.0000\n",
      "  inference_time mean=0.3754s, std=0.0000\n",
      "  accuracy mean=0.590, std=0.000\n",
      "Train model on 89 examples\n",
      "Model trained in 0:00:01.787183\n",
      "\n",
      "(n=128, p=8192):\n",
      "  train_time mean=2.4384s, std=0.0000\n",
      "  inference_time mean=0.5504s, std=0.0000\n",
      "  accuracy mean=0.487, std=0.000\n",
      "Train model on 89 examples\n",
      "Model trained in 0:00:03.519441\n",
      "\n",
      "(n=128, p=16384):\n",
      "  train_time mean=5.5449s, std=0.0000\n",
      "  inference_time mean=0.9022s, std=0.0000\n",
      "  accuracy mean=0.564, std=0.000\n",
      "Train model on 89 examples\n",
      "Model trained in 0:00:07.459505\n",
      "\n",
      "(n=128, p=32768):\n",
      "  train_time mean=13.7612s, std=0.0000\n",
      "  inference_time mean=1.8878s, std=0.0000\n",
      "  accuracy mean=0.436, std=0.000\n",
      "Train model on 89 examples\n",
      "Model trained in 0:00:15.643989\n",
      "\n",
      "(n=128, p=65536):\n",
      "  train_time mean=39.7805s, std=0.0000\n",
      "  inference_time mean=3.5145s, std=0.0000\n",
      "  accuracy mean=0.564, std=0.000\n",
      "Train model on 179 examples\n",
      "Model trained in 0:00:00.105001\n",
      "\n",
      "(n=256, p=128):\n",
      "  train_time mean=0.1516s, std=0.0000\n",
      "  inference_time mean=0.0327s, std=0.0000\n",
      "  accuracy mean=0.571, std=0.000\n",
      "Train model on 179 examples\n",
      "Model trained in 0:00:00.176559\n",
      "\n",
      "(n=256, p=256):\n",
      "  train_time mean=0.2021s, std=0.0000\n",
      "  inference_time mean=0.0326s, std=0.0000\n",
      "  accuracy mean=0.455, std=0.000\n",
      "Train model on 179 examples\n",
      "Model trained in 0:00:00.322859\n",
      "\n",
      "(n=256, p=512):\n",
      "  train_time mean=0.3602s, std=0.0000\n",
      "  inference_time mean=0.0446s, std=0.0000\n",
      "  accuracy mean=0.532, std=0.000\n",
      "Train model on 179 examples\n",
      "Model trained in 0:00:00.601202\n",
      "\n",
      "(n=256, p=1024):\n",
      "  train_time mean=0.6628s, std=0.0000\n",
      "  inference_time mean=0.0699s, std=0.0000\n",
      "  accuracy mean=0.571, std=0.000\n",
      "Train model on 179 examples\n",
      "Model trained in 0:00:01.184042\n",
      "\n",
      "(n=256, p=2048):\n",
      "  train_time mean=1.2879s, std=0.0000\n",
      "  inference_time mean=0.1105s, std=0.0000\n",
      "  accuracy mean=0.455, std=0.000\n",
      "Train model on 179 examples\n",
      "Model trained in 0:00:02.340673\n",
      "\n",
      "(n=256, p=4096):\n",
      "  train_time mean=2.5550s, std=0.0000\n",
      "  inference_time mean=0.1917s, std=0.0000\n",
      "  accuracy mean=0.571, std=0.000\n",
      "Train model on 179 examples\n",
      "Model trained in 0:00:04.707597\n",
      "\n",
      "(n=256, p=8192):\n",
      "  train_time mean=5.3066s, std=0.0000\n",
      "  inference_time mean=0.5910s, std=0.0000\n",
      "  accuracy mean=0.442, std=0.000\n",
      "Train model on 179 examples\n",
      "Model trained in 0:00:09.296474\n",
      "\n",
      "(n=256, p=16384):\n",
      "  train_time mean=11.2065s, std=0.0000\n",
      "  inference_time mean=0.9492s, std=0.0000\n",
      "  accuracy mean=0.481, std=0.000\n",
      "Train model on 179 examples\n",
      "Model trained in 0:00:18.890612\n",
      "\n",
      "(n=256, p=32768):\n",
      "  train_time mean=25.2877s, std=0.0000\n",
      "  inference_time mean=1.9862s, std=0.0000\n",
      "  accuracy mean=0.519, std=0.000\n",
      "Train model on 179 examples\n",
      "Model trained in 0:00:40.498663\n",
      "\n",
      "(n=256, p=65536):\n",
      "  train_time mean=61.8290s, std=0.0000\n",
      "  inference_time mean=3.5055s, std=0.0000\n",
      "  accuracy mean=0.519, std=0.000\n",
      "Train model on 358 examples\n",
      "Model trained in 0:00:00.238514\n",
      "\n",
      "(n=512, p=128):\n",
      "  train_time mean=0.3004s, std=0.0000\n",
      "  inference_time mean=0.0647s, std=0.0000\n",
      "  accuracy mean=0.513, std=0.000\n",
      "Train model on 358 examples\n",
      "Model trained in 0:00:00.426792\n",
      "\n",
      "(n=512, p=256):\n",
      "  train_time mean=0.4690s, std=0.0000\n",
      "  inference_time mean=0.0614s, std=0.0000\n",
      "  accuracy mean=0.506, std=0.000\n",
      "Train model on 358 examples\n",
      "Model trained in 0:00:00.795636\n",
      "\n",
      "(n=512, p=512):\n",
      "  train_time mean=0.8595s, std=0.0000\n",
      "  inference_time mean=0.0809s, std=0.0000\n",
      "  accuracy mean=0.461, std=0.000\n",
      "Train model on 358 examples\n",
      "Model trained in 0:00:01.515099\n",
      "\n",
      "(n=512, p=1024):\n",
      "  train_time mean=1.6017s, std=0.0000\n",
      "  inference_time mean=0.0974s, std=0.0000\n",
      "  accuracy mean=0.506, std=0.000\n",
      "Train model on 358 examples\n",
      "Model trained in 0:00:02.979364\n",
      "\n",
      "(n=512, p=2048):\n",
      "  train_time mean=3.1095s, std=0.0000\n",
      "  inference_time mean=0.1441s, std=0.0000\n",
      "  accuracy mean=0.506, std=0.000\n",
      "Train model on 358 examples\n",
      "Model trained in 0:00:05.806349\n",
      "\n",
      "(n=512, p=4096):\n",
      "  train_time mean=6.0419s, std=0.0000\n",
      "  inference_time mean=0.2339s, std=0.0000\n",
      "  accuracy mean=0.487, std=0.000\n",
      "Train model on 358 examples\n",
      "Model trained in 0:00:11.884597\n",
      "\n",
      "(n=512, p=8192):\n",
      "  train_time mean=12.6984s, std=0.0000\n",
      "  inference_time mean=0.6442s, std=0.0000\n",
      "  accuracy mean=0.487, std=0.000\n",
      "Train model on 358 examples\n",
      "Model trained in 0:00:23.500898\n",
      "\n",
      "(n=512, p=16384):\n",
      "  train_time mean=25.5246s, std=0.0000\n",
      "  inference_time mean=1.0242s, std=0.0000\n",
      "  accuracy mean=0.481, std=0.000\n",
      "Train model on 358 examples\n",
      "Model trained in 0:00:46.875808\n",
      "\n",
      "(n=512, p=32768):\n",
      "  train_time mean=53.3014s, std=0.0000\n",
      "  inference_time mean=1.7749s, std=0.0000\n",
      "  accuracy mean=0.565, std=0.000\n",
      "Train model on 358 examples\n",
      "Model trained in 0:01:39.171541\n",
      "\n",
      "(n=512, p=65536):\n",
      "  train_time mean=122.8557s, std=0.0000\n",
      "  inference_time mean=3.7179s, std=0.0000\n",
      "  accuracy mean=0.474, std=0.000\n",
      "Train model on 716 examples\n",
      "Model trained in 0:00:00.548520\n",
      "\n",
      "(n=1024, p=128):\n",
      "  train_time mean=0.6306s, std=0.0000\n",
      "  inference_time mean=0.1167s, std=0.0000\n",
      "  accuracy mean=0.513, std=0.000\n",
      "Train model on 716 examples\n",
      "Model trained in 0:00:01.018275\n",
      "\n",
      "(n=1024, p=256):\n",
      "  train_time mean=1.0924s, std=0.0000\n",
      "  inference_time mean=0.1198s, std=0.0000\n",
      "  accuracy mean=0.497, std=0.000\n",
      "Train model on 716 examples\n",
      "Model trained in 0:00:01.902898\n",
      "\n",
      "(n=1024, p=512):\n",
      "  train_time mean=1.9921s, std=0.0000\n",
      "  inference_time mean=0.1325s, std=0.0000\n",
      "  accuracy mean=0.516, std=0.000\n",
      "Train model on 716 examples\n",
      "Model trained in 0:00:03.736300\n",
      "\n",
      "(n=1024, p=1024):\n",
      "  train_time mean=3.8507s, std=0.0000\n",
      "  inference_time mean=0.1626s, std=0.0000\n",
      "  accuracy mean=0.519, std=0.000\n",
      "Train model on 716 examples\n",
      "Model trained in 0:00:07.305922\n",
      "\n",
      "(n=1024, p=2048):\n",
      "  train_time mean=7.4728s, std=0.0000\n",
      "  inference_time mean=0.2109s, std=0.0000\n",
      "  accuracy mean=0.516, std=0.000\n",
      "Train model on 716 examples\n",
      "Model trained in 0:00:14.613187\n",
      "\n",
      "(n=1024, p=4096):\n",
      "  train_time mean=14.9193s, std=0.0000\n",
      "  inference_time mean=0.3089s, std=0.0000\n",
      "  accuracy mean=0.481, std=0.000\n",
      "Train model on 716 examples\n",
      "Model trained in 0:00:29.503297\n",
      "\n",
      "(n=1024, p=8192):\n",
      "  train_time mean=30.4849s, std=0.0000\n",
      "  inference_time mean=0.5096s, std=0.0000\n",
      "  accuracy mean=0.497, std=0.000\n",
      "Train model on 716 examples\n",
      "Model trained in 0:00:57.805936\n",
      "\n",
      "(n=1024, p=16384):\n",
      "  train_time mean=60.2723s, std=0.0000\n",
      "  inference_time mean=1.1503s, std=0.0000\n",
      "  accuracy mean=0.510, std=0.000\n",
      "Train model on 716 examples\n",
      "Model trained in 0:01:55.479700\n",
      "\n",
      "(n=1024, p=32768):\n",
      "  train_time mean=122.3559s, std=0.0000\n",
      "  inference_time mean=1.8943s, std=0.0000\n",
      "  accuracy mean=0.526, std=0.000\n",
      "Train model on 716 examples\n",
      "Model trained in 0:04:06.784584\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# YDF Benchmark with Repeats & 3D Plot\n",
    "##############################################\n",
    "\n",
    "\n",
    "def benchmark_ydf(n_vals, d_vals, repeats=7, random_state=42, log_file_path=None):\n",
    "    \"\"\"\n",
    "    KEEPING N_NONZEROS in Proj Matrix constant!\n",
    "\n",
    "    Benchmarks YDF's training & inference time across multiple (n, d) data sizes,\n",
    "    repeating each (n,d) `repeats` times to compute mean & std dev.\n",
    "\n",
    "    Returns a DataFrame with columns:\n",
    "      [n, d,\n",
    "       train_time_mean, train_time_std,\n",
    "       inference_time_mean, inference_time_std,\n",
    "       accuracy_mean, accuracy_std]\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    rows = []\n",
    "\n",
    "    for n in n_vals:\n",
    "        for d in d_vals:\n",
    "            train_times = []\n",
    "            inference_times = []\n",
    "            accuracies = []\n",
    "\n",
    "            for _ in range(repeats):\n",
    "                # 1) Generate random data\n",
    "                X = np.random.randn(n, d)\n",
    "                y = np.random.randint(2, size=n)\n",
    "\n",
    "                # 2) Split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.3, random_state=random_seed, stratify=y\n",
    "                )\n",
    "\n",
    "                # 3) Convert to DF with string column names\n",
    "                cols = [f\"X{i}\" for i in range(d)]\n",
    "                train_df = pd.DataFrame(X_train, columns=cols)\n",
    "                train_df[\"label\"] = y_train\n",
    "\n",
    "                test_df = pd.DataFrame(X_test, columns=cols)\n",
    "                test_df[\"label\"] = y_test\n",
    "\n",
    "                est = ydf.RandomForestLearner(\n",
    "                    label=\"label\",\n",
    "                    random_seed=random_seed,\n",
    "                    split_axis=\"SPARSE_OBLIQUE\",\n",
    "                    num_trees=1000,\n",
    "                    bootstrap_training_dataset=True,\n",
    "                    bootstrap_size_ratio=1.0,\n",
    "                    num_threads=96,\n",
    "                    max_depth=-1,\n",
    "\n",
    "                    sparse_oblique_projection_density_factor=128, # This should finally set n_nonzeros in proj matrix constant (in expectation)\n",
    "\n",
    "                    # sparse_oblique_max_num_projections = 128,\n",
    "                    # sparse_oblique_num_projections_exponent=0.0, # \"\"\" This should make max_num_projections always WIN \"\"\"\n",
    "\n",
    "                    # ''' Should skip computing OOB after training'''\n",
    "                    honest=False,\n",
    "                    compute_oob_performances=False, # Should skip computing OOB after training\n",
    "                )\n",
    "\n",
    "                # 5) Training time\n",
    "                t0 = time.perf_counter()\n",
    "                model = est.train(train_df)\n",
    "                t1 = time.perf_counter()\n",
    "                train_times.append(t1 - t0)\n",
    "\n",
    "                # 6) Inference time\n",
    "                t2 = time.perf_counter()\n",
    "                preds_df = model.predict(test_df)  # prob of label=1\n",
    "                t3 = time.perf_counter()\n",
    "                inference_times.append(t3 - t2)\n",
    "\n",
    "                # Convert probability -> predicted label\n",
    "                preds_np = preds_df#.values.ravel()\n",
    "                pred_labels = (preds_np >= 0.5).astype(int)\n",
    "                acc = accuracy_score(y_test, pred_labels)\n",
    "                accuracies.append(acc)\n",
    "\n",
    "            # Mean/std\n",
    "            train_time_mean = np.mean(train_times)\n",
    "            train_time_std  = np.std(train_times)\n",
    "            inf_time_mean   = np.mean(inference_times)\n",
    "            inf_time_std    = np.std(inference_times)\n",
    "            acc_mean        = np.mean(accuracies)\n",
    "            acc_std         = np.std(accuracies)\n",
    "\n",
    "            msg = (\n",
    "                f\"\\n(n={n}, p={d}):\\n\"\n",
    "                f\"  train_time mean={train_time_mean:.4f}s, std={train_time_std:.4f}\\n\"\n",
    "                f\"  inference_time mean={inf_time_mean:.4f}s, std={inf_time_std:.4f}\\n\"\n",
    "                f\"  accuracy mean={acc_mean:.3f}, std={acc_std:.3f}\"\n",
    "            )\n",
    "\n",
    "            # Print to screen\n",
    "            print(msg)\n",
    "\n",
    "            # Write to file right away, no buffering\n",
    "            if log_file_path:  # NEW\n",
    "                with open(log_file_path, \"a\") as f:  # open in append mode\n",
    "                    f.write(msg + \"\\n\")\n",
    "                    f.flush()  # force flush, ensures content is written\n",
    "\n",
    "            rows.append({\n",
    "                \"n\": n,\n",
    "                \"p\": d,\n",
    "                \"train_time_mean\": train_time_mean,\n",
    "                \"train_time_std\": train_time_std,\n",
    "                \"inference_time_mean\": inf_time_mean,\n",
    "                \"inference_time_std\": inf_time_std,\n",
    "                \"accuracy_mean\": acc_mean,\n",
    "                \"accuracy_std\": acc_std\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_values = [128, 256,512,1024,2048,4096,8192, 16384, 32768, 65536]#, 131072]\n",
    "    p_values = [128,256,512,1024,2048,4096,8192, 16384, 32768, 65536]#, 131072]\n",
    "\n",
    "    perf_logpath = \"../benchmark_results/ydf_benchmark_output.txt\"\n",
    "\n",
    "    # If you want a header first, you can do that outside:\n",
    "    with open(perf_logpath, \"w\") as f:\n",
    "        f.write(\"=== YDF Benchmark Output ===\\n\")\n",
    "        f.flush()\n",
    "\n",
    "    df_bench = benchmark_ydf(n_values, p_values, repeats=1,\n",
    "                               random_state=42, log_file_path=perf_logpath)\n",
    "\n",
    "    summary = \"\\n=== YDF Benchmark Results (Averaged Over Repeats) ===\\n\"\n",
    "    print(summary)\n",
    "    with open(perf_logpath, \"a\") as f:\n",
    "        f.write(summary + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "    print(df_bench)\n",
    "    with open(perf_logpath, \"a\") as f:\n",
    "        f.write(df_bench.to_string(index=False) + \"\\n\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Profiling\n",
    "\n",
    "Profiling in python in useless. Should profile in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\n",
    "\n",
    "import ydf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def generate_ydf_data(n, d, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    X = np.random.randn(n, d)\n",
    "    y = np.random.randint(2, size=n)\n",
    "    cols = [f\"X{i}\" for i in range(d)]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    train_df = pd.DataFrame(X_train, columns=cols)\n",
    "    train_df[\"label\"] = y_train\n",
    "\n",
    "    test_df = pd.DataFrame(X_test, columns=cols)\n",
    "    test_df[\"label\"] = y_test\n",
    "\n",
    "    return train_df, test_df, y_test\n",
    "\n",
    "\n",
    "n, d = 4000, 2048\n",
    "train_df, test_df, y_test = generate_ydf_data(n, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"ydf_random_test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
