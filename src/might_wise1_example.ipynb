{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dcbc36e",
   "metadata": {},
   "source": [
    "# MIGHT example on Wise-1 cancer data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1c657",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import libraries and local functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf5a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency libararies\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# treeple functions\n",
    "from treeple.ensemble import HonestForestClassifier\n",
    "from treeple.tree import ObliqueDecisionTreeClassifier\n",
    "from treeple.stats import build_oob_forest\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e25a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local function for processing cancer data files\n",
    "def get_X_y(f, root=\"./data/\", cohort=[], verbose=False):\n",
    "    df = pd.read_csv(root + f)\n",
    "    non_features = [\n",
    "        \"Run\",\n",
    "        \"Sample\",\n",
    "        \"Library\",\n",
    "        \"Cancer Status\",\n",
    "        \"Tumor type\",\n",
    "        \"Stage\",\n",
    "        \"Library volume (uL)\",\n",
    "        \"Library Volume\",\n",
    "        \"UIDs Used\",\n",
    "        \"Experiment\",\n",
    "        \"P7\",\n",
    "        \"P7 Primer\",\n",
    "        \"MAF\",\n",
    "    ]\n",
    "    sample_ids = df[\"Sample\"]\n",
    "    # if sample is contains \"Run\" column, remove it\n",
    "    for i, sample_id in enumerate(sample_ids):\n",
    "        if \".\" in sample_id:\n",
    "            sample_ids[i] = sample_id.split(\".\")[1]\n",
    "    target = \"Cancer Status\"\n",
    "    y = df[target]\n",
    "    # convert the labels to 0 and 1\n",
    "    y = y.replace(\"Healthy\", 0)\n",
    "    y = y.replace(\"Cancer\", 1)\n",
    "    # remove the non-feature columns if they exist\n",
    "    for col in non_features:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col, axis=1)\n",
    "    nan_cols = df.isnull().all(axis=0).to_numpy()\n",
    "    # drop the columns with all nan values\n",
    "    df = df.loc[:, ~nan_cols]\n",
    "    # if cohort is not None, filter the samples\n",
    "    if cohort is not None:\n",
    "        # filter the rows with cohort1 samples\n",
    "        X = df[sample_ids.isin(cohort)]\n",
    "        y = y[sample_ids.isin(cohort)]\n",
    "    else:\n",
    "        X = df\n",
    "    if \"Wise\" in f:\n",
    "        # replace nans with zero\n",
    "        X = X.fillna(0)\n",
    "    # impute the nan values with the mean of the column\n",
    "    X = X.fillna(X.mean(axis=0))\n",
    "    # check if there are nan values\n",
    "    # nan_rows = X.isnull().any(axis=1)\n",
    "    nan_cols = X.isnull().all(axis=0)\n",
    "    # remove the columns with all nan values\n",
    "    X = X.loc[:, ~nan_cols]\n",
    "    if verbose:\n",
    "        if nan_cols.sum() > 0:\n",
    "            print(f)\n",
    "            print(f\"nan_cols: {nan_cols.sum()}\")\n",
    "            print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "        else:\n",
    "            print(f)\n",
    "            print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    # X = X.dropna()\n",
    "    # y = y.drop(nan_rows.index)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3398e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Process the Wise-1 cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d48c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"./\"\n",
    "N_JOBS = 4\n",
    "N_EST = 100 # 100_000\n",
    "\n",
    "# collect sample data\n",
    "sample_list_file = DIRECTORY + \"AllSamples.MIGHT.Passed.samples.txt\"\n",
    "sample_list = pd.read_csv(sample_list_file, sep=\" \", header=None)\n",
    "sample_list.columns = [\"library\", \"sample_id\", \"cohort\"]\n",
    "\n",
    "# get the sample ids for specific cohorts\n",
    "cohort1 = sample_list[sample_list[\"cohort\"] == \"Cohort1\"][\"sample_id\"]\n",
    "cohort2 = sample_list[sample_list[\"cohort\"] == \"Cohort2\"][\"sample_id\"]\n",
    "PON = sample_list[sample_list[\"cohort\"] == \"PanelOfNormals\"][\"sample_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a73cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the samples for cohort 1\n",
    "X, y = get_X_y(\"WiseCondorX.Wise-1.csv.gz\", root=DIRECTORY, cohort=cohort1)\n",
    "\n",
    "# Save the processed data to a CSV file for use in the Yggdrasil notebook\n",
    "processed_data = X.copy()\n",
    "processed_data[\"Cancer Status\"] = y\n",
    "processed_data.to_csv(\"processed_wise1_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9ee0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352, 2523)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cca2a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run axis-aligned MIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed384d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2514145409986668\n"
     ]
    }
   ],
   "source": [
    "est = HonestForestClassifier(\n",
    "    n_estimators=N_EST,\n",
    "    max_samples=1.6,\n",
    "    max_features=0.3,\n",
    "    bootstrap=True,\n",
    "    stratify=True,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=23,\n",
    "    honest_prior=\"ignore\",\n",
    "    honest_method='prune',\n",
    ")\n",
    "\n",
    "# record start time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# obtain tree level posteriors\n",
    "fitted_est, tree_proba = build_oob_forest(est, X, y)\n",
    "\n",
    "# calculate fitting time\n",
    "end_time = time.perf_counter()\n",
    "time_dif = end_time-start_time\n",
    "\n",
    "print(time_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7e19307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain forest level posteriors\n",
    "forest_proba = np.nanmean(tree_proba, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6049d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d19b1a3f",
   "metadata": {},
   "source": [
    "## Or run oblique MIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e04b695-cec0-4127-ac0f-7e7872012057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency libararies\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# treeple functions\n",
    "from treeple.ensemble import HonestForestClassifier\n",
    "from treeple.tree import ObliqueDecisionTreeClassifier\n",
    "from treeple import ObliqueRandomForestClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d42307",
   "metadata": {},
   "outputs": [],
   "source": [
    "honest_classifier = HonestForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_samples=1.0, # Use all samples for bootstrapping. Use 1.0 instead of 1. 1 means literally 1 sample\n",
    "    max_features=128, # Keep this static\n",
    "    bootstrap=True,\n",
    "    stratify=True,\n",
    "    n_jobs=-1, # Use all resources\n",
    "    random_state=42,\n",
    "    honest_prior=\"ignore\",\n",
    "    honest_method='prune',\n",
    "    tree_estimator=ObliqueDecisionTreeClassifier(),\n",
    "    max_depth = None, # Fully grow trees\n",
    ")\n",
    "\n",
    "oblique_classifier = ObliqueRandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_samples=1.0, # Use all samples for bootstrapping. Use 1.0 instead of 1. 1 means literally 1 sample\n",
    "    max_features=128, # Keep this static\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1, # Use all resources\n",
    "    random_state=42,\n",
    "    max_depth = None, # Fully grow trees\n",
    "    oob_score=False # Don't Inference OOB samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b0cc2",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f4c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from treeple.stats import build_oob_forest\n",
    "\n",
    "\n",
    "def benchmark_might(n_vals, d_vals, classifier, repeats=7, random_state=42, log_file_path=None):  # NEW: pass path\n",
    "    \"\"\"\n",
    "    Benchmarks MIGHT's training & inference time across multiple (n, d) data sizes,\n",
    "    repeating each (n,d) `repeats` times to compute mean & std dev.\n",
    "\n",
    "    Returns a DataFrame with columns:\n",
    "      [\"n\", \"d\",\n",
    "       \"train_time_mean\", \"train_time_std\",\n",
    "       \"inference_time_mean\", \"inference_time_std\",\n",
    "       \"accuracy_mean\", \"accuracy_std\"]\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    results = []\n",
    "\n",
    "    for n in n_vals:\n",
    "        for d in d_vals:\n",
    "            train_times = []\n",
    "            inference_times = []\n",
    "            accuracies = []\n",
    "\n",
    "            for _ in range(repeats):\n",
    "                # 1) Generate random data\n",
    "                X = np.random.randn(n, d)\n",
    "\n",
    "                # TODO Important - Ariel: Testing whether a simple Fortran layout here would improve perf.\n",
    "                X = np.asfortranarray(X)\n",
    "\n",
    "                y = np.random.randint(2, size=n)\n",
    "\n",
    "                # 2) Split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.3, random_state=random_state, stratify=y\n",
    "                )\n",
    "\n",
    "                # 3) Convert to DataFrame. TODO Ariel - Disable this to avoid potentially switching back to C order\n",
    "                # df_train = pd.DataFrame(X_train)\n",
    "                df_train = X_train\n",
    "                # df_test  = pd.DataFrame(X_test)\n",
    "                df_test = X_test\n",
    "\n",
    "                # 4) Build classifier\n",
    "                # You could define this earlier, or globally, etc.\n",
    "                est = classifier\n",
    "\n",
    "                # 5) Training time\n",
    "                t0 = time.perf_counter()\n",
    "                fitted_est, tree_proba = build_oob_forest(est, df_train, y_train)\n",
    "                t1 = time.perf_counter()\n",
    "                train_times.append(t1 - t0)\n",
    "\n",
    "                # 6) Inference time\n",
    "                t2 = time.perf_counter()\n",
    "\n",
    "                # Get predicted probabilities directly from the fitted ensemble\n",
    "                y_proba_test = fitted_est.predict_proba(df_test)[:, 1]\n",
    "\n",
    "                # Convert probabilities to binary predictions\n",
    "                y_pred_test = (y_proba_test >= 0.5).astype(int)\n",
    "\n",
    "                # Measure inference time\n",
    "                t3 = time.perf_counter()\n",
    "                inference_times.append(t3 - t2)\n",
    "\n",
    "                # Compute accuracy\n",
    "                acc = accuracy_score(y_test, y_pred_test)\n",
    "                accuracies.append(acc)\n",
    "\n",
    "                # 7) Accuracy\n",
    "                acc = accuracy_score(y_test, y_pred_test)\n",
    "                accuracies.append(acc)\n",
    "\n",
    "            # Mean/std\n",
    "            train_time_mean = np.mean(train_times)\n",
    "            train_time_std  = np.std(train_times)\n",
    "            inf_time_mean   = np.mean(inference_times)\n",
    "            inf_time_std    = np.std(inference_times)\n",
    "            acc_mean        = np.mean(accuracies)\n",
    "            acc_std         = np.std(accuracies)\n",
    "\n",
    "            msg = (\n",
    "                f\"\\n(n={n}, d={d}):\\n\"\n",
    "                f\"  train_time mean={train_time_mean:.4f}s, std={train_time_std:.4f}\\n\"\n",
    "                f\"  inference_time mean={inf_time_mean:.4f}s, std={inf_time_std:.4f}\\n\"\n",
    "                f\"  accuracy mean={acc_mean:.3f}, std={acc_std:.3f}\"\n",
    "            )\n",
    "\n",
    "            # Print to screen\n",
    "            print(msg)\n",
    "\n",
    "            # Write to file right away, no buffering\n",
    "            if log_file_path:  # NEW\n",
    "                with open(log_file_path, \"a\") as f:  # open in append mode\n",
    "                    f.write(msg + \"\\n\")\n",
    "                    f.flush()  # force flush, ensures content is written\n",
    "\n",
    "            results.append({\n",
    "                \"n\": n,\n",
    "                \"d\": d,\n",
    "                \"train_time_mean\": train_time_mean,\n",
    "                \"train_time_std\": train_time_std,\n",
    "                \"inference_time_mean\": inf_time_mean,\n",
    "                \"inference_time_std\": inf_time_std,\n",
    "                \"accuracy_mean\": acc_mean,\n",
    "                \"accuracy_std\": acc_std,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcccc70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Honest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(n=256, d=128):\n",
      "  train_time mean=5.7586s, std=0.0387\n",
      "  inference_time mean=1.0177s, std=0.0020\n",
      "  accuracy mean=0.515, std=0.058\n",
      "\n",
      "(n=256, d=256):\n",
      "  train_time mean=5.8208s, std=0.0345\n",
      "  inference_time mean=1.4511s, std=0.0024\n",
      "  accuracy mean=0.541, std=0.012\n",
      "\n",
      "(n=256, d=512):\n",
      "  train_time mean=5.9991s, std=0.0835\n",
      "  inference_time mean=2.3131s, std=0.0065\n",
      "  accuracy mean=0.519, std=0.000\n",
      "\n",
      "(n=256, d=1024):\n",
      "  train_time mean=6.0633s, std=0.0238\n",
      "  inference_time mean=3.9988s, std=0.0046\n",
      "  accuracy mean=0.537, std=0.016\n"
     ]
    }
   ],
   "source": [
    "n_values = [128, 256,512,1024,2048,4096,8192, 16384, 32768, 65536, 131072]\n",
    "d_values = [128,256,512,1024,2048,4096,8192, 16384, 32768, 65536, 131072]\n",
    "\n",
    "# If you want a header first, you can do that outside:\n",
    "with open(\"might_benchmark_output.txt\", \"w\") as f:\n",
    "    f.write(\"=== MIGHT Benchmark Output ===\\n\")\n",
    "    f.flush()\n",
    "\n",
    "df_bench = benchmark_might(n_values, d_values, classifier=honest_classifier, repeats=3,\n",
    "                            random_state=42, log_file_path=\"might_benchmark_output.txt\")\n",
    "\n",
    "# Final summary\n",
    "summary = \"\\n=== MIGHT Benchmark Results (Averaged Over Repeats) ===\\n\"\n",
    "print(summary)\n",
    "with open(\"might_benchmark_output.txt\", \"a\") as f:\n",
    "    f.write(summary + \"\\n\")\n",
    "    f.flush()\n",
    "\n",
    "print(df_bench)\n",
    "with open(\"might_benchmark_output.txt\", \"a\") as f:\n",
    "    f.write(df_bench.to_string(index=False) + \"\\n\")\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe992e07",
   "metadata": {},
   "source": [
    "## Oblique, non-Honest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab58511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(n=128, d=128):\n",
      "  train_time mean=1.8768s, std=0.0338\n",
      "  inference_time mean=0.0986s, std=0.0013\n",
      "  accuracy mean=0.444, std=0.032\n",
      "\n",
      "(n=128, d=256):\n",
      "  train_time mean=1.8829s, std=0.0079\n",
      "  inference_time mean=0.1003s, std=0.0013\n",
      "  accuracy mean=0.496, std=0.024\n",
      "\n",
      "(n=128, d=512):\n",
      "  train_time mean=1.9386s, std=0.0532\n",
      "  inference_time mean=0.1047s, std=0.0014\n",
      "  accuracy mean=0.573, std=0.024\n",
      "\n",
      "(n=128, d=1024):\n",
      "  train_time mean=2.0406s, std=0.0244\n",
      "  inference_time mean=0.0981s, std=0.0011\n",
      "  accuracy mean=0.581, std=0.053\n",
      "\n",
      "(n=128, d=2048):\n",
      "  train_time mean=2.2252s, std=0.0386\n",
      "  inference_time mean=0.1008s, std=0.0006\n",
      "  accuracy mean=0.479, std=0.079\n",
      "\n",
      "(n=128, d=4096):\n",
      "  train_time mean=3.7329s, std=0.0957\n",
      "  inference_time mean=0.1124s, std=0.0109\n",
      "  accuracy mean=0.453, std=0.044\n"
     ]
    }
   ],
   "source": [
    "n_values = [128, 256,512,1024,2048,4096,8192, 16384, 32768, 65536]#, 131072]\n",
    "d_values = [128,256,512,1024,2048,4096,8192, 16384, 32768, 65536]#, 131072]\n",
    "\n",
    "perf_logpath = \"../benchmark_results/oblique_treeple_benchmark_output.txt\"\n",
    "\n",
    "# If you want a header first, you can do that outside:\n",
    "with open(perf_logpath, \"w\") as f:\n",
    "    f.write(\"=== OBLIQUE MIGHT Benchmark Output ===\\n\")\n",
    "    f.flush()\n",
    "\n",
    "df_bench = benchmark_might(n_values, d_values, classifier=oblique_classifier, repeats=3,\n",
    "                            random_state=42, log_file_path=perf_logpath)\n",
    "\n",
    "# Final summary\n",
    "summary = \"\\n=== MIGHT Benchmark Results (Averaged Over Repeats) ===\\n\"\n",
    "print(summary)\n",
    "with open(perf_logpath, \"a\") as f:\n",
    "    f.write(summary + \"\\n\")\n",
    "    f.flush()\n",
    "\n",
    "print(df_bench)\n",
    "with open(perf_logpath, \"a\") as f:\n",
    "    f.write(df_bench.to_string(index=False) + \"\\n\")\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823465ba-b2e1-47e1-b3b4-297fa281b017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
