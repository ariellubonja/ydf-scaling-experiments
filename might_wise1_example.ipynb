{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dcbc36e",
   "metadata": {},
   "source": [
    "# MIGHT example on Wise-1 cancer data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1c657",
   "metadata": {},
   "source": [
    "## Import libraries and local functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf5a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency libararies\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# treeple functions\n",
    "from treeple.ensemble import HonestForestClassifier\n",
    "from treeple.tree import ObliqueDecisionTreeClassifier\n",
    "from treeple.stats import build_oob_forest\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e25a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local function for processing cancer data files\n",
    "def get_X_y(f, root=\"./data/\", cohort=[], verbose=False):\n",
    "    df = pd.read_csv(root + f)\n",
    "    non_features = [\n",
    "        \"Run\",\n",
    "        \"Sample\",\n",
    "        \"Library\",\n",
    "        \"Cancer Status\",\n",
    "        \"Tumor type\",\n",
    "        \"Stage\",\n",
    "        \"Library volume (uL)\",\n",
    "        \"Library Volume\",\n",
    "        \"UIDs Used\",\n",
    "        \"Experiment\",\n",
    "        \"P7\",\n",
    "        \"P7 Primer\",\n",
    "        \"MAF\",\n",
    "    ]\n",
    "    sample_ids = df[\"Sample\"]\n",
    "    # if sample is contains \"Run\" column, remove it\n",
    "    for i, sample_id in enumerate(sample_ids):\n",
    "        if \".\" in sample_id:\n",
    "            sample_ids[i] = sample_id.split(\".\")[1]\n",
    "    target = \"Cancer Status\"\n",
    "    y = df[target]\n",
    "    # convert the labels to 0 and 1\n",
    "    y = y.replace(\"Healthy\", 0)\n",
    "    y = y.replace(\"Cancer\", 1)\n",
    "    # remove the non-feature columns if they exist\n",
    "    for col in non_features:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col, axis=1)\n",
    "    nan_cols = df.isnull().all(axis=0).to_numpy()\n",
    "    # drop the columns with all nan values\n",
    "    df = df.loc[:, ~nan_cols]\n",
    "    # if cohort is not None, filter the samples\n",
    "    if cohort is not None:\n",
    "        # filter the rows with cohort1 samples\n",
    "        X = df[sample_ids.isin(cohort)]\n",
    "        y = y[sample_ids.isin(cohort)]\n",
    "    else:\n",
    "        X = df\n",
    "    if \"Wise\" in f:\n",
    "        # replace nans with zero\n",
    "        X = X.fillna(0)\n",
    "    # impute the nan values with the mean of the column\n",
    "    X = X.fillna(X.mean(axis=0))\n",
    "    # check if there are nan values\n",
    "    # nan_rows = X.isnull().any(axis=1)\n",
    "    nan_cols = X.isnull().all(axis=0)\n",
    "    # remove the columns with all nan values\n",
    "    X = X.loc[:, ~nan_cols]\n",
    "    if verbose:\n",
    "        if nan_cols.sum() > 0:\n",
    "            print(f)\n",
    "            print(f\"nan_cols: {nan_cols.sum()}\")\n",
    "            print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "        else:\n",
    "            print(f)\n",
    "            print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    # X = X.dropna()\n",
    "    # y = y.drop(nan_rows.index)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3398e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Process the Wise-1 cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d48c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"./\"\n",
    "N_JOBS = 4\n",
    "N_EST = 100 # 100_000\n",
    "\n",
    "# collect sample data\n",
    "sample_list_file = DIRECTORY + \"AllSamples.MIGHT.Passed.samples.txt\"\n",
    "sample_list = pd.read_csv(sample_list_file, sep=\" \", header=None)\n",
    "sample_list.columns = [\"library\", \"sample_id\", \"cohort\"]\n",
    "\n",
    "# get the sample ids for specific cohorts\n",
    "cohort1 = sample_list[sample_list[\"cohort\"] == \"Cohort1\"][\"sample_id\"]\n",
    "cohort2 = sample_list[sample_list[\"cohort\"] == \"Cohort2\"][\"sample_id\"]\n",
    "PON = sample_list[sample_list[\"cohort\"] == \"PanelOfNormals\"][\"sample_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a73cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the samples for cohort 1\n",
    "X, y = get_X_y(\"WiseCondorX.Wise-1.csv.gz\", root=DIRECTORY, cohort=cohort1)\n",
    "\n",
    "# Save the processed data to a CSV file for use in the Yggdrasil notebook\n",
    "processed_data = X.copy()\n",
    "processed_data[\"Cancer Status\"] = y\n",
    "processed_data.to_csv(\"processed_wise1_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9ee0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352, 2523)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cca2a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run axis-aligned MIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed384d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2514145409986668\n"
     ]
    }
   ],
   "source": [
    "est = HonestForestClassifier(\n",
    "    n_estimators=N_EST,\n",
    "    max_samples=1.6,\n",
    "    max_features=0.3,\n",
    "    bootstrap=True,\n",
    "    stratify=True,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=23,\n",
    "    honest_prior=\"ignore\",\n",
    "    honest_method='prune',\n",
    ")\n",
    "\n",
    "# record start time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# obtain tree level posteriors\n",
    "fitted_est, tree_proba = build_oob_forest(est, X, y)\n",
    "\n",
    "# calculate fitting time\n",
    "end_time = time.perf_counter()\n",
    "time_dif = end_time-start_time\n",
    "\n",
    "print(time_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7e19307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain forest level posteriors\n",
    "forest_proba = np.nanmean(tree_proba, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6049d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d19b1a3f",
   "metadata": {},
   "source": [
    "## Or run oblique MIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e04b695-cec0-4127-ac0f-7e7872012057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency libararies\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# treeple functions\n",
    "from treeple.ensemble import HonestForestClassifier\n",
    "from treeple.tree import ObliqueDecisionTreeClassifier\n",
    "from treeple.stats import build_oob_forest\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d42307",
   "metadata": {},
   "outputs": [],
   "source": [
    "est = HonestForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_samples=1.0, # Use all samples for bootstrapping. Use 1.0 instead of 1. 1 means literally 1 sample\n",
    "    max_features=160, # Keep this static\n",
    "    bootstrap=True,\n",
    "    stratify=True,\n",
    "    n_jobs=-1, # Use all resources\n",
    "    random_state=42,\n",
    "    honest_prior=\"ignore\",\n",
    "    honest_method='prune',\n",
    "    tree_estimator=ObliqueDecisionTreeClassifier(),\n",
    "    max_depth = None, # Fully grow trees\n",
    "    n_threads = 96\n",
    ")\n",
    "\n",
    "# # record start time\n",
    "# start_time = time.perf_counter()\n",
    "\n",
    "# # obtain tree level posteriors\n",
    "# fitted_est, tree_proba = build_oob_forest(est, X, y)\n",
    "\n",
    "# # calculate fitting time\n",
    "# end_time = time.perf_counter()\n",
    "# time_dif = end_time-start_time\n",
    "\n",
    "# print(time_dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b0cc2",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# MIGHT Benchmark with Repeats & 3D Plot\n",
    "##############################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# MIGHT / treeple\n",
    "from treeple.ensemble import HonestForestClassifier\n",
    "from treeple.tree import ObliqueDecisionTreeClassifier\n",
    "from treeple.stats import build_oob_forest\n",
    "\n",
    "\n",
    "def plot_3d_bars(pivot_df, title=\"3D Bar Plot\", zlabel=\"Runtime (s)\"):\n",
    "    \"\"\"\n",
    "    Plots a 3D bar chart from a pivot table DataFrame where:\n",
    "      - pivot_df.index = array of n-values\n",
    "      - pivot_df.columns = array of d-values\n",
    "      - pivot_df.values = numeric quantity (runtime, etc.)\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    n_values = pivot_df.index.to_numpy()\n",
    "    d_values = pivot_df.columns.to_numpy()\n",
    "\n",
    "    n_indices = np.arange(len(n_values))\n",
    "    d_indices = np.arange(len(d_values))\n",
    "\n",
    "    bar_width = 0.7\n",
    "    xs = []\n",
    "    ys = []\n",
    "    dzs = []\n",
    "\n",
    "    for i, n_val in enumerate(n_values):\n",
    "        for j, d_val in enumerate(d_values):\n",
    "            xs.append(j)                      # x = index of d\n",
    "            ys.append(i)                      # y = index of n\n",
    "            dzs.append(pivot_df.iloc[i, j])   # the cell value\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    zs = np.zeros_like(xs)  # bars start at z=0\n",
    "    dx = bar_width * np.ones_like(xs)\n",
    "    dy = bar_width * np.ones_like(xs)\n",
    "    dz = np.array(dzs)\n",
    "\n",
    "    ax.bar3d(xs, ys, zs, dx, dy, dz)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"d (features)\")\n",
    "    ax.set_ylabel(\"n (rows)\")\n",
    "    ax.set_zlabel(zlabel)\n",
    "\n",
    "    # Ticks: integer positions, labeled by actual n and d\n",
    "    ax.set_xticks(d_indices + bar_width / 2)\n",
    "    ax.set_xticklabels(d_values)\n",
    "\n",
    "    ax.set_yticks(n_indices + bar_width / 2)\n",
    "    ax.set_yticklabels(n_values)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def benchmark_might(n_vals, d_vals, repeats=7, random_state=42):\n",
    "    \"\"\"\n",
    "    Benchmarks MIGHT's training & inference time across multiple (n, d) data sizes,\n",
    "    repeating each (n,d) `repeats` times to compute mean & std dev.\n",
    "\n",
    "    Returns a DataFrame with columns:\n",
    "      [\"n\", \"d\",\n",
    "       \"train_time_mean\", \"train_time_std\",\n",
    "       \"inference_time_mean\", \"inference_time_std\",\n",
    "       \"accuracy_mean\", \"accuracy_std\"]\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    results = []\n",
    "\n",
    "    for n in n_vals:\n",
    "        for d in d_vals:\n",
    "            train_times = []\n",
    "            inference_times = []\n",
    "            accuracies = []\n",
    "\n",
    "            for _ in range(repeats):\n",
    "                # 1) Generate random data\n",
    "                X = np.random.randn(n, d)\n",
    "                y = np.random.randint(2, size=n)\n",
    "\n",
    "                # 2) Split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.3, random_state=random_state, stratify=y\n",
    "                )\n",
    "\n",
    "                # 3) Convert to DataFrame\n",
    "                df_train = pd.DataFrame(X_train)\n",
    "                df_test  = pd.DataFrame(X_test)\n",
    "\n",
    "                # 4) Build classifier\n",
    "                # est = HonestForestClassifier(\n",
    "                #     n_estimators=50,\n",
    "                #     max_samples=1.0,\n",
    "                #     max_features=0.3, # TODO Ariel what is this? We want this a fixed integer, not a ratio\n",
    "                #     bootstrap=True,\n",
    "                #     stratify=True,\n",
    "                #     n_jobs=-1,\n",
    "                #     random_state=random_state,\n",
    "                #     honest_prior=\"ignore\",\n",
    "                #     honest_method=\"prune\",\n",
    "                #     tree_estimator=ObliqueDecisionTreeClassifier()\n",
    "                # )\n",
    "                global est\n",
    "\n",
    "                # 5) Training time\n",
    "                t0 = time.perf_counter()\n",
    "                fitted_est, tree_proba = build_oob_forest(est, df_train, y_train)\n",
    "                t1 = time.perf_counter()\n",
    "                train_times.append(t1 - t0)\n",
    "\n",
    "                # 6) Inference time\n",
    "                t2 = time.perf_counter()\n",
    "                test_tree_proba = []\n",
    "                for tree in fitted_est.estimators_:\n",
    "                    p = tree.predict_proba(df_test)  # shape (len(X_test), 2)\n",
    "                    test_tree_proba.append(p)\n",
    "                test_tree_proba = np.stack(test_tree_proba, axis=0)\n",
    "                avg_proba = test_tree_proba.mean(axis=0)\n",
    "                y_proba_test = avg_proba[:, 1]\n",
    "                y_pred_test = (y_proba_test >= 0.5).astype(int)\n",
    "                t3 = time.perf_counter()\n",
    "                inference_times.append(t3 - t2)\n",
    "\n",
    "                # 7) Accuracy\n",
    "                acc = accuracy_score(y_test, y_pred_test)\n",
    "                accuracies.append(acc)\n",
    "\n",
    "            # Mean/std\n",
    "            train_time_mean = np.mean(train_times)\n",
    "            train_time_std  = np.std(train_times)\n",
    "            inf_time_mean   = np.mean(inference_times)\n",
    "            inf_time_std    = np.std(inference_times)\n",
    "            acc_mean        = np.mean(accuracies)\n",
    "            acc_std         = np.std(accuracies)\n",
    "\n",
    "            print(f\"\\n(n={n}, d={d}):\")\n",
    "            print(f\"  train_time mean={train_time_mean:.4f}s, std={train_time_std:.4f}\")\n",
    "            print(f\"  inference_time mean={inf_time_mean:.4f}s, std={inf_time_std:.4f}\")\n",
    "            print(f\"  accuracy mean={acc_mean:.3f}, std={acc_std:.3f}\")\n",
    "\n",
    "            results.append({\n",
    "                \"n\": n,\n",
    "                \"d\": d,\n",
    "                \"train_time_mean\": train_time_mean,\n",
    "                \"train_time_std\": train_time_std,\n",
    "                \"inference_time_mean\": inf_time_mean,\n",
    "                \"inference_time_std\": inf_time_std,\n",
    "                \"accuracy_mean\": acc_mean,\n",
    "                \"accuracy_std\": acc_std,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example grid\n",
    "    n_values = [125, 250, 500, 1000, 2000, 4000, 8000]\n",
    "    d_values = [10, 20, 40, 80, 160, 320, 640]\n",
    "\n",
    "    df_bench = benchmark_might(n_values, d_values, repeats=7, random_state=42)\n",
    "\n",
    "    print(\"\\n=== MIGHT Benchmark Results (Averaged Over Repeats) ===\")\n",
    "    print(df_bench)\n",
    "\n",
    "    # Build pivot tables for train & inference means\n",
    "    pivot_train = df_bench.pivot(index=\"n\", columns=\"d\", values=\"train_time_mean\")\n",
    "    pivot_infer = df_bench.pivot(index=\"n\", columns=\"d\", values=\"inference_time_mean\")\n",
    "\n",
    "    # ---- 1) Heatmap: Training Time\n",
    "    plt.figure()\n",
    "    plt.imshow(pivot_train, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"MIGHT Training Time Mean (sec)\")\n",
    "    plt.xticks(range(len(d_values)), d_values)\n",
    "    plt.yticks(range(len(n_values)), n_values)\n",
    "    plt.xlabel(\"Features (d)\")\n",
    "    plt.ylabel(\"Rows (n)\")\n",
    "    plt.show()\n",
    "\n",
    "    # ---- 2) Heatmap: Inference Time\n",
    "    plt.figure()\n",
    "    plt.imshow(pivot_infer, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"MIGHT Inference Time Mean (sec)\")\n",
    "    plt.xticks(range(len(d_values)), d_values)\n",
    "    plt.yticks(range(len(n_values)), n_values)\n",
    "    plt.xlabel(\"Features (d)\")\n",
    "    plt.ylabel(\"Rows (n)\")\n",
    "    plt.show()\n",
    "\n",
    "    # ---- 3) 3D \"Cityscape\" for Training Time\n",
    "    plot_3d_bars(\n",
    "        pivot_train,\n",
    "        title=\"MIGHT Training Time (3D Bar)\",\n",
    "        zlabel=\"Train time (s)\"\n",
    "    )\n",
    "\n",
    "    # ---- 4) 3D \"Cityscape\" for Inference Time\n",
    "    plot_3d_bars(\n",
    "        pivot_infer,\n",
    "        title=\"MIGHT Inference Time (3D Bar)\",\n",
    "        zlabel=\"Inference time (s)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d2fc6",
   "metadata": {},
   "source": [
    "\n",
    "- [ ] Comparable YDF trees to MIGHT? MIGHT code is being ran right? Why is 0.5 probability?\n",
    "- [ ] Try on a Intel machine\n",
    "    - Focus on Training Time. Focus on YDF.\n",
    "- [ ] Scale up to overwhelm cache size\n",
    "- [ ] Figure out why accuracy is a coin flip\n",
    "\n",
    "\n",
    "\n",
    "Hi Randal,\n",
    "\n",
    "The corresponding parameters to set in ydf are:\n",
    "```\n",
    "    bootstrap_size_ratio = max_samples\n",
    "    num_candidate_attributes_ratio = max_features\n",
    "    random_seed = random_state\n",
    "    num_threads = n_jobs\n",
    "```\n",
    "\n",
    "max_samples=1.6 is based on our attempt to give more samples to each tree in the forest, and max_samples>1.0 would not be available in other forest learners. I couldn't find stratification parameters in ydf, so that's only in treeple, too. max_features=0.3 does represent 0.3 of features to consider during splitting. It is a conventional number (another one is square root of the number of features), so we can set that for both forests.\n",
    "\n",
    "So, to make the comparison fair, after excluding the parts ydf couldn't do, we recommend using these 2 setups:\n",
    "\n",
    "treeple_model = HonestForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_samples=1.0,\n",
    "    max_features=0.3,\n",
    "    bootstrap=True,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=23,\n",
    "    honest_prior=\"ignore\",\n",
    "    tree_estimator=ObliqueDecisionTreeClassifier(),\n",
    ")\n",
    "\n",
    "ydf_model = ydf.RandomForestLearner(\n",
    "    label=\"Cancer Status\",\n",
    "    split_axis=\"SPARSE_OBLIQUE\",\n",
    "    num_trees=1000, \n",
    "    max_depth=-1,  \n",
    "    honest=True,\n",
    "    bootstrap_training_dataset=True\n",
    "    bootstrap_size_ratio=1.0,\n",
    "    num_threads=N_JOBS,\n",
    "    num_candidate_attributes_ratio=0.3,\n",
    "    random_seed=23,\n",
    ")\n",
    "\n",
    "They should have close matching parameters for comparisons. Please let us know if you have other questions. Thank you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec85030",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
